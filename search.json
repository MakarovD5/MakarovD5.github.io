[{"title":"大数据笔记","url":"/2022/05/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AC%94%E8%AE%B0/","content":"大数据当前观看集数：47简介星号（*）代表我自己的注释\n环境配置基础Linux配置\n关防火墙\n\nhost配置，主机名设置\n\nssh免密\n\n集群时间同步\nntpdate ntp4.aliyun.com\n\n\nJava配置\n创建统一的工作目录\n\n解压jdk包\n\n配置环境变量\n记得之后更新下配置文件\nsource /etc/profile\n\n查看java安装路径\nwhich java\n\n\n\n用scp复制到别的服务器，jdk和环境配置都复制，结合FinalShell全部会话命令\n\n# 主机名 cat /etc/hostname# hosts映射vim /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.88.151 node1.itcast.cn node1192.168.88.152 node2.itcast.cn node2192.168.88.153 node3.itcast.cn node3# JDK 1.8安装  上传 jdk-8u241-linux-x64.tar.gz到/export/server/目录下cd /export/server/tar zxvf jdk-8u241-linux-x64.tar.gz\t#配置环境变量\tvim /etc/profile\texport JAVA_HOME=/export/server/jdk1.8.0_241\texport PATH=$PATH:$JAVA_HOME/bin\texport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\t#重新加载环境变量文件\tsource /etc/profile# 集群时间同步ntpdate ntp5.aliyun.com# 防火墙关闭firewall-cmd --state\t#查看防火墙状态systemctl stop firewalld.service  #停止firewalld服务systemctl disable firewalld.service  #开机禁用firewalld服务# ssh免密登录（只需要配置node1至node1、node2、node3即可）\t#node1生成公钥私钥 (一路回车)\tssh-keygen  \t#node1配置免密登录到node1 node2 node3\tssh-copy-id node1\tssh-copy-id node2\tssh-copy-id node3\n\n\nHadoop配置\n解压编译好的安装包（正常情况需要自己从源码编译或者官方编译后的）\n为什么要编译：hadoop官方自己编译好的程序在链接动态链接库时容易出现问题，需要自己编译符合自己机器的程序\n\n配置文件：&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop\n第一类1个：hadoop-env.sh\n第二类4个：xxxx-site.xml ,site表示的是用户定义的配置，会覆盖default中的默认配置。\n​\t\t\t\t\t\tcore-site.xml 核心模块配置\n​\t\t\t\t\t\thdfs-site.xml hdfs文件系统模块配置\n​\t\t\t\t\t\tmapred-site.xml MapReduce模块配置\n​\t\t\t\t\t\tyarn-site.xml yarn模块配置\n 第三类1个：workers\n\n默认配置文件内容：\nhttps://hadoop.apache.org/docs/r3.3.1/hadoop-project-dist/hadoop-common/core-default.xml\n\n\n将Hadoop添加到环境变量\n\nscp分发同步Hadoop安装包和环境变量profile文件\nscp -r hadoop-3.3.0 root@node2:$PWD\n\n记得之后更新下配置文件\nsource /etc/profile\n\n首次启动HDFS时，必须对其进行格式化操作。首次启动要格式化namenode，format只能进行一次 后续不再需要，只在第一台机器\nhdfs namenode -format\n\n\n\n\nhadoop-env.sh\n#文件最后添加export JAVA_HOME=/export/server/jdk1.8.0_241export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root \n\ncore-site.xml\n&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://node1:8020&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置Hadoop本地保存数据路径 --&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HDFS web UI用户身份 --&gt;&lt;property&gt;&lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;!-- 整合hive 用户代理设置 --&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!-- 文件系统垃圾桶保存时间 --&gt;&lt;property&gt;&lt;name&gt;fs.trash.interval&lt;/name&gt;&lt;value&gt;1440&lt;/value&gt;&lt;/property&gt;\n\nhdfs-site.xml\n&lt;!-- 设置SNN进程运行机器位置信息 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;&lt;value&gt;node2:9868&lt;/value&gt;&lt;/property&gt;\n\nmapred-site.xml\n&lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;node1:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务器web端地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;&lt;value&gt;node1:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;&lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.map.env&lt;/name&gt;&lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.reduce.env&lt;/name&gt;&lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;\n\nyarn-site.xml\n&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;&lt;property&gt;\t&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\t&lt;value&gt;node1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施物理内存限制 --&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启日志聚集 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置yarn历史服务器地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.log.server.url&lt;/name&gt;&lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史日志保存的时间 7天 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;&lt;value&gt;604800&lt;/value&gt;&lt;/property&gt;\n\nworkers\nnode1.itcast.cnnode2.itcast.cnnode3.itcast.cn\n\n将hadoop添加到环境变量（3台机器）\nvim /etc/profileexport HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinsource /etc/profile#别忘了scp给其他两台机器哦\n\n\nHadoop最初使用shell脚本一键启停前提：配置好机器之间的SSH免密登录和workers文件。\nHDFS集群：start-dfs.shstop-dfs.shYARN集群：start-yarn.shstop-yarn.shHadoop集群：start-all.shstop-all.sh \n启动出现问题记得看日志&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;logs&#x2F;\nWeb UIHDFS集群：http://namenode_host:9870\nYARN集群：http://resourcemanager_host:8088\n重要特性\n主从架构\n一主多从\n\n分块存储\n默认128M，从物理上将文件分割，存储由datanode完成\n\n副本机制\n默认有2个副本，即每个块在3个位置分别存储\n\n元数据记录\n由namenode完成，记录文件信息和块的储存位置\n\n抽象统一的目录树结构（namespace）\n层次型文件组织结构，即目录树\n\n\n模拟实现分布式文件存储1.如何解决海量数据存的下的–分布式存储\n2.如何解决海量数据文件查询便捷—-元数据记录\n3.如何解决大文件传输效率慢—-分块存储\n4.如何解决硬件故障数据丢失–副本机制\n5.如何解决用户查询视角统一规整–抽象目录树结构\nHDFSHDFS设计目标\\1.   HDFS集群由很多的服务器组成，而每一个机器都与可能会出现故障。HDFS为了能够进行故障检测、快速恢复等。\n\\2.   HDFS主要适合去做批量数据出来，相对于数据请求时的反应时间，HDFS更倾向于保障吞吐量。（*数据处理往往一周一次、一月一次等等，不需要考虑访问当时的反应速度）\n\\3.   典型的HDFS中的文件大小是GB到TB，HDFS比较适合存储大文件\n\\4.   HDFS很多时候是以： Write-One-Read-Many来应用的，一旦在HDFS创建一个文件，写入完后就不需要修改了。（*比如记录昨天的天气以后一定不会再修改了）\n\\5.   移动计算的代价比之移动数据的代价低。一个应用请求的计算，离它操作的数据越近就越高效。将计算移动到数据附近，比之将数据移动到应用所在显然更好。\nShellhadoop fs [generic options]\n文件系统协议hadoop fs -ls file:&#x2F;&#x2F;&#x2F; #操作本地文件系统hadoop fs -ls hdfs:&#x2F;&#x2F;node1:8020&#x2F; #操作HDFS分布式文件系统hadoop fs -ls &#x2F; #直接根目录，没有指定协议 将加载读取fs.defaultFS值\nhadoop dfs 只能操作HDFS文件系统（包括与Local FS间的操作），不过已经Deprecated；hdfs dfs 只能操作HDFS文件系统相关（包括与Local FS间的操作）,常用；hadoop fs 可操作任意文件系统，不仅仅是hdfs文件系统，使用范围更广；目前版本来看，官方最终推荐使用的是hadoop fs。当然hdfs dfs在市面上的使用也比较多。可以通过hadoop fs -help命令来查看每个命令的详细用法。(*看完help就相当于看完所有的shell命令，极大提高功力)\n常用命令创建文件夹hadoop fs -mkdir [-p]  …path 为待创建的目录-p选项的行为与Unix mkdir -p非常相似，它会沿着路径创建父目录。\nhadoop fs -mkdir /itcast\n\n查看指定目录下内容hadoop fs -ls [-h] [-R] [ …]path 指定目录路径-h 人性化显示文件size-R 递归查看指定目录及其子目录\n上传文件到HDFS指定目录下 hadoop fs -put [-f] [-p]  … -f 覆盖目标文件（已存在下）-p 保留访问和修改时间，所有权和权限。localsrc 本地文件系统（客户端所在机器）dst 目标文件系统（HDFS）\nhadoop fs -put zookeeper.out /itcasthadoop fs -put file:///etc/profile hdfs://node1:8020/itcast\n\nmoveFromLocal和put参数类似，但是源文件localsrc拷贝之后自身被删除hdfs dfs **-**moveFromLocal &lt;**localsrc**&gt; &lt;**dst**&gt;\n查看HDFS文件内容hadoop fs -cat  …读取指定文件全部内容，显示在标准输出控制台。注意：对于大文件内容读取，慎重。（*可用head和tail命令）\nhadoop fs -cat /itcast/zookeeper.out\n\nhead 命令\nhdfs dfs -head URI\ntail 命令\nhdfs dfs -tail [-f] URI\n-f选项表示数据只要有变化也会输出到控制台。\n下载HDFS文件hadoop fs -get [-f] [-p]  … 下载文件到本地文件系统指定目录，localdst必须是目录-f 覆盖目标文件（已存在下）-p 保留访问和修改时间，所有权和权限。\n[root@node2 ~]# mkdir test[root@node2 ~]# cd test/[root@node2 test]# lltotal 0[root@node2 test]# hadoop fs -get /itcast/zookeeper.out ./（*./代表当前路径）[root@node2 test]# lltotal 20-rw-r--r-- 1 root root 18213 Aug 18 17:54 zookeeper.out\n\n合并下载getmerge\nhadoop fs -getmerge [-nl] [-skip-empty-file]      下载多个文件合并到本地文件系统的一个文件中。    -nl选项表示在每个文件末尾添加换行符    -skip-empty-file跳过空文件\n拷贝HDFS文件hadoop fs -cp [-f]  … -f 覆盖目标文件（已存在下）\n[root@node3 ~]# hadoop fs -cp /small/1.txt /itcast[root@node3 ~]# hadoop fs -cp /small/1.txt /itcast/666.txt #重命令[root@node3 ~]# hadoop fs -ls /itcastFound 4 items-rw-r--r-- 3 root supergroup 2 2021-08-18 17:58 /itcast/1.txt-rw-r--r-- 3 root supergroup 2 2021-08-18 17:59 /itcast/666.txt\n\n追加数据到HDFS文件中hadoop fs -appendToFile  … 将所有给定本地文件的内容追加到给定dst文件。dst如果文件不存在，将创建该文件。如果为-，则输入为从标准输入中读取。\n#追加内容到文件尾部 appendToFile[root@node3 ~]# echo 1 &gt;&gt; 1.txt[root@node3 ~]# echo 2 &gt;&gt; 2.txt [root@node3 ~]# echo 3 &gt;&gt; 3.txt [root@node3 ~]# hadoop fs -appendToFile *.txt /1.txt（*若1.txt有内容则追加到其后面，没有这个文件就新建这个文件）[root@node3 ~]# hadoop fs -cat /1.txt123\n\nHDFS数据移动操作hadoop fs -mv  … 移动文件到指定文件夹下可以使用该命令移动数据，重命名文件的名称\n查看HDFS磁盘使用情况hdfs dfs -df [-h] URI [URI …]\n[root@node1 ~]# hdfs dfs -df -h /Filesystem                      Size   Used  Available  Use%hdfs://node1.itcast.cn:9820  346.6 G  2.1 G    236.7 G    1%\n\n显示目录中所有文件大小du 命令\n显示目录中所有文件大小，当只指定一个文件时，显示此文件的大小。\n语法格式：hdfs dfs -du [-s] [-h] [-v] [-x]  URI [URI …]  \n命令选项：-s：表示显示文件长度的汇总摘要，而不是单个文件的摘要。-h：选项将以“人类可读”的方式格式化文件大小-v：选项将列名显示为标题行。-x：选项将从结果计算中排除快照。 \n[root@node1 ~]# hdfs dfs -du -s -h -v /source/weibo/SIZE    DISK_SPACE_CONSUMED_WITH_ALL_REPLICAS  FULL_PATH_NAME64.2 M  192.6 M                                /source/weibo\n\n修改HDFS文件副本个数hadoop fs -setrep [-R] [-w]   …    修改指定文件的副本个数。    -R表示递归 修改文件夹下及其所有    -w 客户端是否等待副本修改完毕。    副本数\n（*最好提前规划好副本数，不然后面在进行这个setrep操作会很浪费时间资源）\nhadoop fs -setrep -w 2 /tmp/caixukun_dirtydata.csv\n\n\n\n命令官方指导文档Apache Hadoop 3.3.0 – Overview\nHDFS Java 客户端 API客户端核心类Configuration：该类的对象封转了客户端或者服务器的配置\nFileSystem：该类的对象是一个文件系统对象，可以用该对象的一些方法来对文件进行操作，通过FileSystem的静态方法get获得该对象。\nFileSystem fs = FileSystem.get(conf)\n\n\nget方法从conf中的一个参数 fs.defaultFS的配置值判断具体是什么类型的文件系统。如果我们的代码中没有指定fs.defaultFS，并且工程classpath下也没有给定相应的配置，conf中的默认值就来自于hadoop的jar包中的core-default.xml，默认值为： file:&#x2F;&#x2F;&#x2F;，则获取的将不是一个DistributedFileSystem的实例，而是一个本地文件系统的客户端对象。\n\n配置Maven配置pom.xml\n配置阿里源\nmaven-compiler-plugin插件报错：安装maven仓库里有该插件的maven版本，安装后一定要重启idea\n（*需不需要重启idea我自己总结一条规律：是不是自行懂了）\n连接HDFS用单元测试的方式连接，添加@Before和@After\n修改默认用户默认以windwos的用户登录，需要修改为root\n//设置客户端身份 以具备权限在HDFS操作System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;);\n\n创建文件夹mkdir()\n上传文件copyFromLocalFile()\n下载文件copyToLocalFile()\n下载报错\n错误提示：    找不到winutils.exe、HADOOP_HOME没有设置原因：    Hadoop访问windows本地文件系统，要求Windows上的本地库能正常工作。    其中Hadoop使用某些Windows API来实现类似posix的文件访问权限。    上述功能需要在hadoop.dll和winutils.exe来实现。解决：    下载Hadoop源码在windows平台编译，编译出windows本地库。然后配置Hadoop环境变量。HADOOP_HOME&#x3D;C:\\soft\\hadoop-3.1.4path&#x3D;;%HADOOP_HOME%\\bin\nLog4JLog4j具有三个主要组件\nLogger(日志记录器)Logger控制日志的输出级别与日志是否输出；\nAppender（输出端）Appender指定日志的输出方式（ConsoleAppender控制台、FileAppender文件、JDBCAppender等）\nLayout（日志格式化器）Layout控制日志信息的输出格式（simple格式、HTML格式、PatternLayout自定义格式）\n\n日志级别Log4J 在 org.apache.log4j.Level 类中定义了OFF、FATAL、ERROR、WARN、INFO、DEBUG、TRACE、ALL八种日志级别\nERROR &gt; WARN &gt; INFO &gt; DEBUG\n\n\n\n目录\n说明\n\n\n\nERROR\n发生错误事件，但仍不影响系统的继续运行\n\n\nWARN\n警告，即潜在的错误情形\n\n\nINFO\n一般在粗粒度级别上，强调应用程序的运行全程\n\n\nDEBUG\n一般用于细粒度级别上，对调试应用程序非常有帮助\n\n\n程序中使用Log4j\n项目中引入log4j的jar包\n添加配置文件log4j.properties\n代码中使用\n\n项目中引入log4j的jar包hadoop自带log4j，可以不用自己再安装，在依赖项的hadoop-common里\n配置文件log4j.properties可以在Hadoop安装路径里拿取：&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop&#x2F;\n#ConsoleAppender表示控制台，log4j.appender.Console=org.apache.log4j.ConsoleAppender#自定义格式log4j.appender.Console.layout=org.apache.log4j.PatternLayout#格式形式为时间，进程等等。。。log4j.appender.Console.layout.ConversionPattern=%d [%t] %p [%c] - %m%n#DEBUG表示级别，只显示DEBUG及以上的级别，如果为info，则debug及以下级别的不会输出。warn等等同理#Consle只是个名字，起其他任何名字都行log4j.rootLogger=DEBUG,Console\n\nConversionPattern的格式：\n%p: 输出日志信息优先级，即DEBUG，INFO，WARN，ERROR，FATAL,%d: 输出日志时间点的日期或时间，默认格式为ISO8601，也可以在其后指定格式，比如：%d{yyyy-MM-dd HH:mm:ss,SSS}，输出类似：2011-10-18 22:10:28,921 %r: 输出自应用启动到输出该log信息耗费的毫秒数 %c: 输出日志信息所属的类目，通常就是所在类的全名 %t: 输出产生该日志事件的线程名 %l: 输出日志事件的发生位置，相当于%C.%M(%F:%L)的组合,包括类目名、发生的线程，以及在代码中的行数。 %x: 输出和当前线程相关联的NDC(嵌套诊断环境),尤其用到像java servlets这样的多客户多线程的应用中。 %%: 输出一个”%”字符 %F: 输出日志消息产生时所在的文件名称 %L: 输出代码中的行号 %m: 输出代码中指定的消息,产生的日志具体信息 %n: 输出一个回车换行符，Windows平台为”\\r\\n”，Unix平台为”\\n”输出日志信息换行 \n代码中使用Logger logger = Logger.getLogger(对哪个类的记录的类的类名.class)\n\nGoogle-option创建实体类，创建命令行对应public字段，每个字段加@Option\npublic class ServerOptions extends OptionsBase &#123;  @Option(      name = &quot;help&quot;,      abbrev = &#x27;h&#x27;,      help = &quot;Prints usage info.&quot;,      defaultValue = &quot;true&quot;    )  public boolean help;&#125;\n\n使用：\nOptionsParser parser = OptionsParser.newOptionsParser(ServerOptions.class);parser.parseAndExitUponError(args);ServerOptions options = parser.getOptions(ServerOptions.class);\n\n舆情数据上报案例环境准备Google-option\nLog4J\nFileSystem类的使用，非常频繁\n实现生成数据采集任务实现步骤：    1.判断原始数据目录是否存在    2.读取原始数据目录下的所有文件    3.判断待上传目录是否存在，不存在则创建一个    4.创建任务目录（目录名称：task_年月日时分秒_任务状态）    5.遍历待上传的文件，在待上传目录生成一个willDoing文件    6.将待移动的文件添加到willDoing文件中\n知识点：\n1、try-catch的catch部分改为用log4j输出到控制台：（*为什么要再抛一个新的异常目前不清楚，可能为打印出具体错误行数栈）\n在catch子句中可以抛出一个异常， 这样做的目的是 改变异常类型；强烈建议使用这种包装技术， 这样可以让用户抛出子系统中的高级异常， 而不会丢失原始异常的小细节；\nLogger.error(e.getMessage(), e);throw new RuntimeException(e.getMessage());\n\n2、lambda函数：在文章尾部其他领域java部分\n3、如果路径不存在，新建路径\nFileUtils.forceMkdirParent(tempDir);\n\n4、FileUtils的使用\n实现执行数据上报任务实现步骤：\n\n读取待上传目录的willDoing任务文件，注意过滤COPY和DONE后的任务文件夹\n\n\n遍历读取任务文件，开始上传\n\n a)\t将任务文件修改为COPY，表示正在处理中 b)\t获取任务的日期 c)\t判断HDFS目标上传目录是否存在，不存在则创建 d)\t读取任务文件 e)\t按照换行符切分 f)\t上传每一个文件,调用HDFSUtils进行数据文件上传 g)\t上传成功后，将COPY后缀修改为_DONE\n\n(*思路总结，source是从网上爬的数据，按任务分类整理好挪到pending下，再统一上传到hdfs上)\nHadoop基准测试写入基准测试hadoop jar /export/server/hadoop-3.1.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar  TestDFSIO -write -nrFiles 10  -fileSize 10MB\n\n读取基准测试hadoop jar /export/server/hadoop-3.1.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar  TestDFSIO -read -nrFiles 10 -fileSize 10MB\n\n清除测试数据hadoop jar /export/server/hadoop-3.1.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar   TestDFSIO -clean\n\n测试结果各项数值的意义\nNumber of files：生成 mapTask 数量，一般是集群中 CPU 核数 -1，我们测试虚拟机就按照实际的物理内存 -1 分配即可\nTotal MBytes processed：单个 map 处理的文件大小\nThroughput mb/sec：单个 mapTak 的吞吐量\n\n计算方式：处理的总文件大小 &#x2F; 每一个 mapTask 写数据的时间累加 集群整体吞吐量：生成 mapTask 数量 * 单个 mapTak 的吞吐量\n\nAverage IO rate mb/sec：平均 mapTak 的吞吐量\n\n计算方式：每个 mapTask 处理文件大小 &#x2F; 每一个 mapTask 写数据的时间全部相加除以 task 数量\n\nIO rate std deviation：方差、反映各个 mapTask 处理的差值，越小越均衡\n\nHDFS工作流程与机制HDFS集群角色与职责NameNode\nNameNode成为了访问HDFS的唯一入口。\nNameNode仅存储HDFS的元数据：文件系统中所有文件的目录树，并跟踪整个集群中的文件，不存储实际数据。\nNameNode知道HDFS中任何给定文件的块列表及其位置。使用此信息NameNode知道如何从块中构建文件。\nNameNode不持久化存储每个文件中各个块所在的datanode的位置信息，这些信息会在系统启动时从DataNode重建。\nNameNode是Hadoop集群中的单点故障。\nNameNode所在机器通常会配置有大量内存（RAM）。\n\nDataNode\nDataNode是Hadoop HDFS中的从角色，负责具体的数据块存储。决定了HDFS集群的整体数据存储能力。\nDataNode负责最终数据块block的存储。是集群的从角色，也称为Slave。\nDataNode启动时，会将自己注册到NameNode并汇报自己负责持有的块列表。\n当某个DataNode关闭时，不会影响数据的可用性。 NameNode将安排由其他DataNode管理的块进行副本复制。\nDataNode所在机器通常配置有大量的硬盘空间，因为实际数据存储在DataNode中。\n\nSecondaryNameNode\nSecondary NameNode充当NameNode的辅助节点，但不能替代NameNode。\n主要是帮助主角色进行元数据文件的合并动作。可以通俗的理解为主角色的“秘书”。\n\nHDFS读写数据流程*读写流程面试常考，面试前可以看源码解析了解流程\nPipeline管道\npipeline是线性传输，顺序的沿着一个方向传输，这样能够充分利用每个机器的带宽，避免网络瓶颈和高延迟时 的连接，最小化推送所有数据的延时。\n\nACK应答响应\nACK (Acknowledge character）即是确认字符，在数据通信中，接收方发给发送方的一种传输类控制字符。表示发来的数据已确认接收无误。\n在HDFS pipeline管道传输数据的过程中，传输的反方向会进行ACK校验，确保数据传输安全。\n\n默认3副本存储策略\n默认副本存储策略是由BlockPlacementPolicyDefault指定。\n第一块副本：优先客户端本地，否则随机第二块副本：不同于第一块副本的不同机架。（*机架就是一个衣柜样的机柜）第三块副本：第二块副本相同机架不同机器。\n\nMapReduce其他领域Javalambda的使用// 读取原始数据目录下的所有文件File[] allSourceDataFile = sourceDir.listFiles(f -&gt; &#123;    // 判断文件格式是否以 weibo_data_ 开头    String fileName = f.getName();    if (fileName.startsWith(&quot;weibo_data_&quot;)) &#123;        return true;    &#125;    return false;&#125;);\n\n这里的listFiles()括号里的内容可以理解为：\n首先，listFiles的参数形式有三种：空，FileFilter接口，FilenameFilter接口\n//分别看看接口的样子public interface FileFilter &#123;    boolean accept(File pathname);&#125;public interface FilenameFilter &#123;    boolean accept(File dir, String name);&#125;\n\nlambda函数实现了FileFilter接口为什么是FileFilter而不是FilenameFilter：因为lambda只有一个参数f，而FilenameFilter需要两个参数。\nFileFilter中只有一个抽象方法，返回的是Boolean型，所以lambda函数就实现了这个方法。\n总的来看：lambda返回的是FileFilter接口的实例对象，只有一个f参数，且f为File类型参数，lambda的{}内实现了accept方法，该方法返回的是boolean型。\n使用Lambda时，要记住的就两点：\n\nLambda返回的是接口的实例对象\n有没有参数、参数有多少个、需不需要有返回值、返回值的类型是什么—-&gt;选择自己合适的函数式接口\n（*Lambda只能实现函数式接口，即只有一个抽象方法的接口，用@FunctionalInterface标注函数式接口，所以如果接口有两个抽象方法就无法使用lambda）\n\nFileUtils的使用 FileUtils 是 Apache Commons IO 的一部分\n&lt;dependency&gt;    &lt;groupId&gt;commons-io&lt;/groupId&gt;    &lt;artifactId&gt;commons-io&lt;/artifactId&gt;    &lt;version&gt;2.6&lt;/version&gt;&lt;/dependency&gt;\n\nFileUtils.moveFileWhen the destination file is on another file system, do a “copy and delete”.\n捕获异常+再次抛出异常与异常链在catch子句中可以抛出一个异常， 这样做的目的是 改变异常类型；强烈建议使用这种包装技术， 这样可以让用户抛出子系统中的高级异常， 而不会丢失原始异常的小细节；\n&#125; catch (IOException e) &#123;//            e.printStackTrace();            logger.error(e.getMessage(),e);            throw new RuntimeException(e);        &#125;\n\nJAVA IO创建文件\nnew File(String path);\nnew File(File father, String chilend);\nnew File(String father, String child);\n\n获取文件信息\ngetName();\ngetAbsolutePath();\ngetParent();\nlength();&#x2F;&#x2F;字节数，英文1个字节，汉字3个字节\nexists();\nisFile();\nisDirectory();\n\n目录操作\ndelete();&#x2F;&#x2F;删除文件和空文件夹，文件夹里有文件就不行，目录\nmkdir();\nmkdirs();&#x2F;&#x2F;递归创建\n\nLinuxproc目录&#x2F;proc是一个位于内存中的伪文件系统(in-memory pseudo-file system)。该目录下保存的不是真正的文件和目录，而是一些“运行时”信息，如系统内存、磁盘io、设备挂载信息和硬件配置信息等。\nlsmod命令就是cat &#x2F;proc&#x2F;modules命令的别名，lspci命令是cat &#x2F;proc&#x2F;pci命令的别名。\n\n&#x2F;proc&#x2F;loadavg 保存了系统负载的平均值，其前三列分别表示最近1分钟、5分钟及15分的平均负载。反映了当前系统的繁忙情况。\n&#x2F;proc&#x2F;meminfo 当前内存使用的统计信息，常由free命令使用；可以使用文件查看命令直接读取此文件，其内容显示为两列，前者为统计属性，后者为对应的值；\n&#x2F;proc&#x2F;diskstats 磁盘设备的磁盘I&#x2F;O统计信息列表;\n&#x2F;proc&#x2F;net&#x2F;dev 网络流入流出的统计信息，包括接收包的数量、发送包的数量，发送数据包时的错误和冲突情况等。\n&#x2F;proc&#x2F;cmdline 在启动时传递至内核的启动参数，通常由grub启动管理工具进行传递；\n&#x2F;proc&#x2F;devices 系统已经加载的所有块设备和字符设备的信息；\n&#x2F;proc&#x2F;mounts 系统中当前挂载的所有文件系统；\n&#x2F;proc&#x2F;partitions 块设备每个分区的主设备号（major）和次设备号（minor）等信息，同时包括每个分区所包含的块（block）数目；\n&#x2F;proc&#x2F;uptime 系统上次启动以来的运行时间；\n&#x2F;proc&#x2F;version 当前系统运行的内核版本号，在作者的Debian系统中，还会显示系统安装的gcc版本；\n&#x2F;proc&#x2F;vmstat 当前系统虚拟内存的统计数据。\n\nscp命令：基于ssh远程拷贝scp 是 secure copy 的缩写, scp 是 linux 系统下基于 ssh 登陆进行安全的远程文件拷贝命令。可以结合SSH免密，并设置host地址，把IP数字设置成简短的英文名\n参数-r： 递归复制整个目录。复制目录时要加-r\nscp -r jdk1.8.0_241 root@node2:/export/server/\n\nFinalShell的命令编辑器能发送到全部会话IDEApom报错如果手动修改了本地文件，pom报错，重启一下idea，因为idea无法动态识别\n快速选中一行一、鼠标连续点三下二、end键将光标移到行尾 ， ctrl+w 选中行三、end键将光标移到行尾 ， shift + home 选中行四、home 键 光标移到行首、然后 点击shift +end\nMavenmaven插件版本不需要和maven版本对应，每个maven插件都有自己的版本个人成长PEST分析法从政治、经济、社会、技术因素去分析企业管理经营的问题。可以应用到对个人的分析\n其他命令行英文缩写CLI命令行界面（英语：command-line interface，缩写：CLI）\n几个磁盘架构知识磁盘阵列RAID2、3、4较少实际应用，它们大多只在研究领域有实作。\n◇RAID 0\n\n优点：使用 n 颗硬盘，即可拥有将近 n 倍的读写效能。\n缺点：数据安全性较低，同组数组中任一硬盘发生问题就会造成数据遗失。\n硬盘数量：最少 2 个。\n\n\n◇RAID 1\n\n优点：安全性依照数组里的实体硬盘数量倍数成长。\n缺点：空间利用率是所有 RAID 中最没有效率的。\n硬盘数量：最少 2 个。\n\n\n◇RAID 5\n\n优点：兼顾空间利用率与安全性。\n缺点：需要额外的运算资源，仅能忍受 1 个硬盘损毁。\n硬盘数量：至少 3 个。\n\n\n◇RAID 6\n\n优点：容错硬盘数量比 RAID 5 多 1 颗。\n缺点：运算量比 RAID 5 大、空间利用率比 RAID 5 低。\n硬盘数量：至少 4 个。\n\n\nDAS，NAS，SAN对比\n\n\n\n","categories":["大数据"],"tags":["大数据"]}]