[{"title":"大数据笔记","url":"/2022/05/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AC%94%E8%AE%B0/","content":"大数据当前观看集数：47简介星号（*）代表我自己的注释\n环境配置基础Linux配置\n关防火墙\n\nhost配置，主机名设置\n\nssh免密\n\n集群时间同步\nntpdate ntp4.aliyun.com\n\n\nJava配置\n创建统一的工作目录\n\n解压jdk包\n\n配置环境变量\n记得之后更新下配置文件\nsource /etc/profile\n\n查看java安装路径\nwhich java\n\n\n\n用scp复制到别的服务器，jdk和环境配置都复制，结合FinalShell全部会话命令\n\n# 主机名 cat /etc/hostname# hosts映射vim /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.88.151 node1.itcast.cn node1192.168.88.152 node2.itcast.cn node2192.168.88.153 node3.itcast.cn node3# JDK 1.8安装  上传 jdk-8u241-linux-x64.tar.gz到/export/server/目录下cd /export/server/tar zxvf jdk-8u241-linux-x64.tar.gz\t#配置环境变量\tvim /etc/profile\texport JAVA_HOME=/export/server/jdk1.8.0_241\texport PATH=$PATH:$JAVA_HOME/bin\texport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\t#重新加载环境变量文件\tsource /etc/profile# 集群时间同步ntpdate ntp5.aliyun.com# 防火墙关闭firewall-cmd --state\t#查看防火墙状态systemctl stop firewalld.service  #停止firewalld服务systemctl disable firewalld.service  #开机禁用firewalld服务# ssh免密登录（只需要配置node1至node1、node2、node3即可）\t#node1生成公钥私钥 (一路回车)\tssh-keygen  \t#node1配置免密登录到node1 node2 node3\tssh-copy-id node1\tssh-copy-id node2\tssh-copy-id node3\n\n\nHadoop配置\n解压编译好的安装包（正常情况需要自己从源码编译或者官方编译后的）\n为什么要编译：hadoop官方自己编译好的程序在链接动态链接库时容易出现问题，需要自己编译符合自己机器的程序\n\n配置文件：&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop\n第一类1个：hadoop-env.sh\n第二类4个：xxxx-site.xml ,site表示的是用户定义的配置，会覆盖default中的默认配置。\n​\t\t\t\t\t\tcore-site.xml 核心模块配置\n​\t\t\t\t\t\thdfs-site.xml hdfs文件系统模块配置\n​\t\t\t\t\t\tmapred-site.xml MapReduce模块配置\n​\t\t\t\t\t\tyarn-site.xml yarn模块配置\n 第三类1个：workers\n\n默认配置文件内容：\nhttps://hadoop.apache.org/docs/r3.3.1/hadoop-project-dist/hadoop-common/core-default.xml\n\n\n将Hadoop添加到环境变量\n\nscp分发同步Hadoop安装包和环境变量profile文件\nscp -r hadoop-3.3.0 root@node2:$PWD\n\n记得之后更新下配置文件\nsource /etc/profile\n\n首次启动HDFS时，必须对其进行格式化操作。首次启动要格式化namenode，format只能进行一次 后续不再需要，只在第一台机器\nhdfs namenode -format\n\n\n\n\nhadoop-env.sh\n#文件最后添加export JAVA_HOME=/export/server/jdk1.8.0_241export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root \n\ncore-site.xml\n&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://node1:8020&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置Hadoop本地保存数据路径 --&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HDFS web UI用户身份 --&gt;&lt;property&gt;&lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;!-- 整合hive 用户代理设置 --&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!-- 文件系统垃圾桶保存时间 --&gt;&lt;property&gt;&lt;name&gt;fs.trash.interval&lt;/name&gt;&lt;value&gt;1440&lt;/value&gt;&lt;/property&gt;\n\nhdfs-site.xml\n&lt;!-- 设置SNN进程运行机器位置信息 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;&lt;value&gt;node2:9868&lt;/value&gt;&lt;/property&gt;\n\nmapred-site.xml\n&lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;node1:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务器web端地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;&lt;value&gt;node1:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;&lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.map.env&lt;/name&gt;&lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.reduce.env&lt;/name&gt;&lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;\n\nyarn-site.xml\n&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;&lt;property&gt;\t&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\t&lt;value&gt;node1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施物理内存限制 --&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启日志聚集 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置yarn历史服务器地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.log.server.url&lt;/name&gt;&lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史日志保存的时间 7天 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;&lt;value&gt;604800&lt;/value&gt;&lt;/property&gt;\n\nworkers\nnode1.itcast.cnnode2.itcast.cnnode3.itcast.cn\n\n将hadoop添加到环境变量（3台机器）\nvim /etc/profileexport HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinsource /etc/profile#别忘了scp给其他两台机器哦\n\n\nHadoop最初使用shell脚本一键启停前提：配置好机器之间的SSH免密登录和workers文件。\nHDFS集群：start-dfs.shstop-dfs.shYARN集群：start-yarn.shstop-yarn.shHadoop集群：start-all.shstop-all.sh \n启动出现问题记得看日志&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;logs&#x2F;\nWeb UIHDFS集群：http://namenode_host:9870\nYARN集群：http://resourcemanager_host:8088\n重要特性\n主从架构\n一主多从\n\n分块存储\n默认128M，从物理上将文件分割，存储由datanode完成\n\n副本机制\n默认有2个副本，即每个块在3个位置分别存储\n\n元数据记录\n由namenode完成，记录文件信息和块的储存位置\n\n抽象统一的目录树结构（namespace）\n层次型文件组织结构，即目录树\n\n\n模拟实现分布式文件存储1.如何解决海量数据存的下的–分布式存储\n2.如何解决海量数据文件查询便捷—-元数据记录\n3.如何解决大文件传输效率慢—-分块存储\n4.如何解决硬件故障数据丢失–副本机制\n5.如何解决用户查询视角统一规整–抽象目录树结构\nHDFSHDFS设计目标\\1.   HDFS集群由很多的服务器组成，而每一个机器都与可能会出现故障。HDFS为了能够进行故障检测、快速恢复等。\n\\2.   HDFS主要适合去做批量数据出来，相对于数据请求时的反应时间，HDFS更倾向于保障吞吐量。（*数据处理往往一周一次、一月一次等等，不需要考虑访问当时的反应速度）\n\\3.   典型的HDFS中的文件大小是GB到TB，HDFS比较适合存储大文件\n\\4.   HDFS很多时候是以： Write-One-Read-Many来应用的，一旦在HDFS创建一个文件，写入完后就不需要修改了。（*比如记录昨天的天气以后一定不会再修改了）\n\\5.   移动计算的代价比之移动数据的代价低。一个应用请求的计算，离它操作的数据越近就越高效。将计算移动到数据附近，比之将数据移动到应用所在显然更好。\nShellhadoop fs [generic options]\n文件系统协议hadoop fs -ls file:&#x2F;&#x2F;&#x2F; #操作本地文件系统hadoop fs -ls hdfs:&#x2F;&#x2F;node1:8020&#x2F; #操作HDFS分布式文件系统hadoop fs -ls &#x2F; #直接根目录，没有指定协议 将加载读取fs.defaultFS值\nhadoop dfs 只能操作HDFS文件系统（包括与Local FS间的操作），不过已经Deprecated；hdfs dfs 只能操作HDFS文件系统相关（包括与Local FS间的操作）,常用；hadoop fs 可操作任意文件系统，不仅仅是hdfs文件系统，使用范围更广；目前版本来看，官方最终推荐使用的是hadoop fs。当然hdfs dfs在市面上的使用也比较多。可以通过hadoop fs -help命令来查看每个命令的详细用法。(*看完help就相当于看完所有的shell命令，极大提高功力)\n常用命令创建文件夹hadoop fs -mkdir [-p]  …path 为待创建的目录-p选项的行为与Unix mkdir -p非常相似，它会沿着路径创建父目录。\nhadoop fs -mkdir /itcast\n\n查看指定目录下内容hadoop fs -ls [-h] [-R] [ …]path 指定目录路径-h 人性化显示文件size-R 递归查看指定目录及其子目录\n上传文件到HDFS指定目录下 hadoop fs -put [-f] [-p]  … -f 覆盖目标文件（已存在下）-p 保留访问和修改时间，所有权和权限。localsrc 本地文件系统（客户端所在机器）dst 目标文件系统（HDFS）\nhadoop fs -put zookeeper.out /itcasthadoop fs -put file:///etc/profile hdfs://node1:8020/itcast\n\nmoveFromLocal和put参数类似，但是源文件localsrc拷贝之后自身被删除hdfs dfs **-**moveFromLocal &lt;**localsrc**&gt; &lt;**dst**&gt;\n查看HDFS文件内容hadoop fs -cat  …读取指定文件全部内容，显示在标准输出控制台。注意：对于大文件内容读取，慎重。（*可用head和tail命令）\nhadoop fs -cat /itcast/zookeeper.out\n\nhead 命令\nhdfs dfs -head URI\ntail 命令\nhdfs dfs -tail [-f] URI\n-f选项表示数据只要有变化也会输出到控制台。\n下载HDFS文件hadoop fs -get [-f] [-p]  … 下载文件到本地文件系统指定目录，localdst必须是目录-f 覆盖目标文件（已存在下）-p 保留访问和修改时间，所有权和权限。\n[root@node2 ~]# mkdir test[root@node2 ~]# cd test/[root@node2 test]# lltotal 0[root@node2 test]# hadoop fs -get /itcast/zookeeper.out ./（*./代表当前路径）[root@node2 test]# lltotal 20-rw-r--r-- 1 root root 18213 Aug 18 17:54 zookeeper.out\n\n合并下载getmerge\nhadoop fs -getmerge [-nl] [-skip-empty-file]      下载多个文件合并到本地文件系统的一个文件中。    -nl选项表示在每个文件末尾添加换行符    -skip-empty-file跳过空文件\n拷贝HDFS文件hadoop fs -cp [-f]  … -f 覆盖目标文件（已存在下）\n[root@node3 ~]# hadoop fs -cp /small/1.txt /itcast[root@node3 ~]# hadoop fs -cp /small/1.txt /itcast/666.txt #重命令[root@node3 ~]# hadoop fs -ls /itcastFound 4 items-rw-r--r-- 3 root supergroup 2 2021-08-18 17:58 /itcast/1.txt-rw-r--r-- 3 root supergroup 2 2021-08-18 17:59 /itcast/666.txt\n\n追加数据到HDFS文件中hadoop fs -appendToFile  … 将所有给定本地文件的内容追加到给定dst文件。dst如果文件不存在，将创建该文件。如果为-，则输入为从标准输入中读取。\n#追加内容到文件尾部 appendToFile[root@node3 ~]# echo 1 &gt;&gt; 1.txt[root@node3 ~]# echo 2 &gt;&gt; 2.txt [root@node3 ~]# echo 3 &gt;&gt; 3.txt [root@node3 ~]# hadoop fs -appendToFile *.txt /1.txt（*若1.txt有内容则追加到其后面，没有这个文件就新建这个文件）[root@node3 ~]# hadoop fs -cat /1.txt123\n\nHDFS数据移动操作hadoop fs -mv  … 移动文件到指定文件夹下可以使用该命令移动数据，重命名文件的名称\n查看HDFS磁盘使用情况hdfs dfs -df [-h] URI [URI …]\n[root@node1 ~]# hdfs dfs -df -h /Filesystem                      Size   Used  Available  Use%hdfs://node1.itcast.cn:9820  346.6 G  2.1 G    236.7 G    1%\n\n显示目录中所有文件大小du 命令\n显示目录中所有文件大小，当只指定一个文件时，显示此文件的大小。\n语法格式：hdfs dfs -du [-s] [-h] [-v] [-x]  URI [URI …]  \n命令选项：-s：表示显示文件长度的汇总摘要，而不是单个文件的摘要。-h：选项将以“人类可读”的方式格式化文件大小-v：选项将列名显示为标题行。-x：选项将从结果计算中排除快照。 \n[root@node1 ~]# hdfs dfs -du -s -h -v /source/weibo/SIZE    DISK_SPACE_CONSUMED_WITH_ALL_REPLICAS  FULL_PATH_NAME64.2 M  192.6 M                                /source/weibo\n\n修改HDFS文件副本个数hadoop fs -setrep [-R] [-w]   …    修改指定文件的副本个数。    -R表示递归 修改文件夹下及其所有    -w 客户端是否等待副本修改完毕。    副本数\n（*最好提前规划好副本数，不然后面在进行这个setrep操作会很浪费时间资源）\nhadoop fs -setrep -w 2 /tmp/caixukun_dirtydata.csv\n\n\n\n命令官方指导文档Apache Hadoop 3.3.0 – Overview\nHDFS Java 客户端 API客户端核心类Configuration：该类的对象封转了客户端或者服务器的配置\nFileSystem：该类的对象是一个文件系统对象，可以用该对象的一些方法来对文件进行操作，通过FileSystem的静态方法get获得该对象。\nFileSystem fs = FileSystem.get(conf)\n\n\nget方法从conf中的一个参数 fs.defaultFS的配置值判断具体是什么类型的文件系统。如果我们的代码中没有指定fs.defaultFS，并且工程classpath下也没有给定相应的配置，conf中的默认值就来自于hadoop的jar包中的core-default.xml，默认值为： file:&#x2F;&#x2F;&#x2F;，则获取的将不是一个DistributedFileSystem的实例，而是一个本地文件系统的客户端对象。\n\n配置Maven配置pom.xml\n配置阿里源\nmaven-compiler-plugin插件报错：安装maven仓库里有该插件的maven版本，安装后一定要重启idea\n（*需不需要重启idea我自己总结一条规律：是不是自行懂了）\n连接HDFS用单元测试的方式连接，添加@Before和@After\n修改默认用户默认以windwos的用户登录，需要修改为root\n//设置客户端身份 以具备权限在HDFS操作System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;);\n\n创建文件夹mkdir()\n上传文件copyFromLocalFile()\n下载文件copyToLocalFile()\n下载报错\n错误提示：    找不到winutils.exe、HADOOP_HOME没有设置原因：    Hadoop访问windows本地文件系统，要求Windows上的本地库能正常工作。    其中Hadoop使用某些Windows API来实现类似posix的文件访问权限。    上述功能需要在hadoop.dll和winutils.exe来实现。解决：    下载Hadoop源码在windows平台编译，编译出windows本地库。然后配置Hadoop环境变量。HADOOP_HOME&#x3D;C:\\soft\\hadoop-3.1.4path&#x3D;;%HADOOP_HOME%\\bin\nLog4JLog4j具有三个主要组件\nLogger(日志记录器)Logger控制日志的输出级别与日志是否输出；\nAppender（输出端）Appender指定日志的输出方式（ConsoleAppender控制台、FileAppender文件、JDBCAppender等）\nLayout（日志格式化器）Layout控制日志信息的输出格式（simple格式、HTML格式、PatternLayout自定义格式）\n\n日志级别Log4J 在 org.apache.log4j.Level 类中定义了OFF、FATAL、ERROR、WARN、INFO、DEBUG、TRACE、ALL八种日志级别\nERROR &gt; WARN &gt; INFO &gt; DEBUG\n\n\n\n目录\n说明\n\n\n\nERROR\n发生错误事件，但仍不影响系统的继续运行\n\n\nWARN\n警告，即潜在的错误情形\n\n\nINFO\n一般在粗粒度级别上，强调应用程序的运行全程\n\n\nDEBUG\n一般用于细粒度级别上，对调试应用程序非常有帮助\n\n\n程序中使用Log4j\n项目中引入log4j的jar包\n添加配置文件log4j.properties\n代码中使用\n\n项目中引入log4j的jar包hadoop自带log4j，可以不用自己再安装，在依赖项的hadoop-common里\n配置文件log4j.properties可以在Hadoop安装路径里拿取：&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop&#x2F;\n#ConsoleAppender表示控制台，log4j.appender.Console=org.apache.log4j.ConsoleAppender#自定义格式log4j.appender.Console.layout=org.apache.log4j.PatternLayout#格式形式为时间，进程等等。。。log4j.appender.Console.layout.ConversionPattern=%d [%t] %p [%c] - %m%n#DEBUG表示级别，只显示DEBUG及以上的级别，如果为info，则debug及以下级别的不会输出。warn等等同理#Consle只是个名字，起其他任何名字都行log4j.rootLogger=DEBUG,Console\n\nConversionPattern的格式：\n%p: 输出日志信息优先级，即DEBUG，INFO，WARN，ERROR，FATAL,%d: 输出日志时间点的日期或时间，默认格式为ISO8601，也可以在其后指定格式，比如：%d{yyyy-MM-dd HH:mm:ss,SSS}，输出类似：2011-10-18 22:10:28,921 %r: 输出自应用启动到输出该log信息耗费的毫秒数 %c: 输出日志信息所属的类目，通常就是所在类的全名 %t: 输出产生该日志事件的线程名 %l: 输出日志事件的发生位置，相当于%C.%M(%F:%L)的组合,包括类目名、发生的线程，以及在代码中的行数。 %x: 输出和当前线程相关联的NDC(嵌套诊断环境),尤其用到像java servlets这样的多客户多线程的应用中。 %%: 输出一个”%”字符 %F: 输出日志消息产生时所在的文件名称 %L: 输出代码中的行号 %m: 输出代码中指定的消息,产生的日志具体信息 %n: 输出一个回车换行符，Windows平台为”\\r\\n”，Unix平台为”\\n”输出日志信息换行 \n代码中使用Logger logger = Logger.getLogger(对哪个类的记录的类的类名.class)\n\nGoogle-option创建实体类，创建命令行对应public字段，每个字段加@Option\npublic class ServerOptions extends OptionsBase &#123;  @Option(      name = &quot;help&quot;,      abbrev = &#x27;h&#x27;,      help = &quot;Prints usage info.&quot;,      defaultValue = &quot;true&quot;    )  public boolean help;&#125;\n\n使用：\nOptionsParser parser = OptionsParser.newOptionsParser(ServerOptions.class);parser.parseAndExitUponError(args);ServerOptions options = parser.getOptions(ServerOptions.class);\n\n舆情数据上报案例环境准备Google-option\nLog4J\nFileSystem类的使用，非常频繁\n实现生成数据采集任务实现步骤：    1.判断原始数据目录是否存在    2.读取原始数据目录下的所有文件    3.判断待上传目录是否存在，不存在则创建一个    4.创建任务目录（目录名称：task_年月日时分秒_任务状态）    5.遍历待上传的文件，在待上传目录生成一个willDoing文件    6.将待移动的文件添加到willDoing文件中\n知识点：\n1、try-catch的catch部分改为用log4j输出到控制台：（*为什么要再抛一个新的异常目前不清楚，可能为打印出具体错误行数栈）\n在catch子句中可以抛出一个异常， 这样做的目的是 改变异常类型；强烈建议使用这种包装技术， 这样可以让用户抛出子系统中的高级异常， 而不会丢失原始异常的小细节；\nLogger.error(e.getMessage(), e);throw new RuntimeException(e.getMessage());\n\n2、lambda函数：在文章尾部其他领域java部分\n3、如果路径不存在，新建路径\nFileUtils.forceMkdirParent(tempDir);\n\n4、FileUtils的使用\n实现执行数据上报任务实现步骤：\n\n读取待上传目录的willDoing任务文件，注意过滤COPY和DONE后的任务文件夹\n\n\n遍历读取任务文件，开始上传\n\n a)\t将任务文件修改为COPY，表示正在处理中 b)\t获取任务的日期 c)\t判断HDFS目标上传目录是否存在，不存在则创建 d)\t读取任务文件 e)\t按照换行符切分 f)\t上传每一个文件,调用HDFSUtils进行数据文件上传 g)\t上传成功后，将COPY后缀修改为_DONE\n\n(*思路总结，source是从网上爬的数据，按任务分类整理好挪到pending下，再统一上传到hdfs上)\nHadoop基准测试写入基准测试hadoop jar /export/server/hadoop-3.1.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar  TestDFSIO -write -nrFiles 10  -fileSize 10MB\n\n读取基准测试hadoop jar /export/server/hadoop-3.1.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar  TestDFSIO -read -nrFiles 10 -fileSize 10MB\n\n清除测试数据hadoop jar /export/server/hadoop-3.1.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar   TestDFSIO -clean\n\n测试结果各项数值的意义\nNumber of files：生成 mapTask 数量，一般是集群中 CPU 核数 -1，我们测试虚拟机就按照实际的物理内存 -1 分配即可\nTotal MBytes processed：单个 map 处理的文件大小\nThroughput mb/sec：单个 mapTak 的吞吐量\n\n计算方式：处理的总文件大小 &#x2F; 每一个 mapTask 写数据的时间累加 集群整体吞吐量：生成 mapTask 数量 * 单个 mapTak 的吞吐量\n\nAverage IO rate mb/sec：平均 mapTak 的吞吐量\n\n计算方式：每个 mapTask 处理文件大小 &#x2F; 每一个 mapTask 写数据的时间全部相加除以 task 数量\n\nIO rate std deviation：方差、反映各个 mapTask 处理的差值，越小越均衡\n\nHDFS工作流程与机制HDFS集群角色与职责NameNode\nNameNode成为了访问HDFS的唯一入口。\nNameNode仅存储HDFS的元数据：文件系统中所有文件的目录树，并跟踪整个集群中的文件，不存储实际数据。\nNameNode知道HDFS中任何给定文件的块列表及其位置。使用此信息NameNode知道如何从块中构建文件。\nNameNode不持久化存储每个文件中各个块所在的datanode的位置信息，这些信息会在系统启动时从DataNode重建。\nNameNode是Hadoop集群中的单点故障。\nNameNode所在机器通常会配置有大量内存（RAM）。\n\nDataNode\nDataNode是Hadoop HDFS中的从角色，负责具体的数据块存储。决定了HDFS集群的整体数据存储能力。\nDataNode负责最终数据块block的存储。是集群的从角色，也称为Slave。\nDataNode启动时，会将自己注册到NameNode并汇报自己负责持有的块列表。\n当某个DataNode关闭时，不会影响数据的可用性。 NameNode将安排由其他DataNode管理的块进行副本复制。\nDataNode所在机器通常配置有大量的硬盘空间，因为实际数据存储在DataNode中。\n\nSecondaryNameNode\nSecondary NameNode充当NameNode的辅助节点，但不能替代NameNode。\n主要是帮助主角色进行元数据文件的合并动作。可以通俗的理解为主角色的“秘书”。\n\nHDFS读写数据流程*读写流程面试常考，面试前可以看源码解析了解流程\nPipeline管道\npipeline是线性传输，顺序的沿着一个方向传输，这样能够充分利用每个机器的带宽，避免网络瓶颈和高延迟时 的连接，最小化推送所有数据的延时。\n\nACK应答响应\nACK (Acknowledge character）即是确认字符，在数据通信中，接收方发给发送方的一种传输类控制字符。表示发来的数据已确认接收无误。\n在HDFS pipeline管道传输数据的过程中，传输的反方向会进行ACK校验，确保数据传输安全。\n\n默认3副本存储策略\n默认副本存储策略是由BlockPlacementPolicyDefault指定。\n第一块副本：优先客户端本地，否则随机第二块副本：不同于第一块副本的不同机架。（*机架就是一个衣柜样的机柜）第三块副本：第二块副本相同机架不同机器。\n\nMapReduce其他领域Javalambda的使用// 读取原始数据目录下的所有文件File[] allSourceDataFile = sourceDir.listFiles(f -&gt; &#123;    // 判断文件格式是否以 weibo_data_ 开头    String fileName = f.getName();    if (fileName.startsWith(&quot;weibo_data_&quot;)) &#123;        return true;    &#125;    return false;&#125;);\n\n这里的listFiles()括号里的内容可以理解为：\n首先，listFiles的参数形式有三种：空，FileFilter接口，FilenameFilter接口\n//分别看看接口的样子public interface FileFilter &#123;    boolean accept(File pathname);&#125;public interface FilenameFilter &#123;    boolean accept(File dir, String name);&#125;\n\nlambda函数实现了FileFilter接口为什么是FileFilter而不是FilenameFilter：因为lambda只有一个参数f，而FilenameFilter需要两个参数。\nFileFilter中只有一个抽象方法，返回的是Boolean型，所以lambda函数就实现了这个方法。\n总的来看：lambda返回的是FileFilter接口的实例对象，只有一个f参数，且f为File类型参数，lambda的{}内实现了accept方法，该方法返回的是boolean型。\n使用Lambda时，要记住的就两点：\n\nLambda返回的是接口的实例对象\n有没有参数、参数有多少个、需不需要有返回值、返回值的类型是什么—-&gt;选择自己合适的函数式接口\n（*Lambda只能实现函数式接口，即只有一个抽象方法的接口，用@FunctionalInterface标注函数式接口，所以如果接口有两个抽象方法就无法使用lambda）\n\nFileUtils的使用 FileUtils 是 Apache Commons IO 的一部分\n&lt;dependency&gt;    &lt;groupId&gt;commons-io&lt;/groupId&gt;    &lt;artifactId&gt;commons-io&lt;/artifactId&gt;    &lt;version&gt;2.6&lt;/version&gt;&lt;/dependency&gt;\n\nFileUtils.moveFileWhen the destination file is on another file system, do a “copy and delete”.\n捕获异常+再次抛出异常与异常链在catch子句中可以抛出一个异常， 这样做的目的是 改变异常类型；强烈建议使用这种包装技术， 这样可以让用户抛出子系统中的高级异常， 而不会丢失原始异常的小细节；\n&#125; catch (IOException e) &#123;//            e.printStackTrace();            logger.error(e.getMessage(),e);            throw new RuntimeException(e);        &#125;\n\nJAVA IO创建文件\nnew File(String path);\nnew File(File father, String chilend);\nnew File(String father, String child);\n\n获取文件信息\ngetName();\ngetAbsolutePath();\ngetParent();\nlength();&#x2F;&#x2F;字节数，英文1个字节，汉字3个字节\nexists();\nisFile();\nisDirectory();\n\n目录操作\ndelete();&#x2F;&#x2F;删除文件和空文件夹，文件夹里有文件就不行，目录\nmkdir();\nmkdirs();&#x2F;&#x2F;递归创建\n\nLinuxproc目录&#x2F;proc是一个位于内存中的伪文件系统(in-memory pseudo-file system)。该目录下保存的不是真正的文件和目录，而是一些“运行时”信息，如系统内存、磁盘io、设备挂载信息和硬件配置信息等。\nlsmod命令就是cat &#x2F;proc&#x2F;modules命令的别名，lspci命令是cat &#x2F;proc&#x2F;pci命令的别名。\n\n&#x2F;proc&#x2F;loadavg 保存了系统负载的平均值，其前三列分别表示最近1分钟、5分钟及15分的平均负载。反映了当前系统的繁忙情况。\n&#x2F;proc&#x2F;meminfo 当前内存使用的统计信息，常由free命令使用；可以使用文件查看命令直接读取此文件，其内容显示为两列，前者为统计属性，后者为对应的值；\n&#x2F;proc&#x2F;diskstats 磁盘设备的磁盘I&#x2F;O统计信息列表;\n&#x2F;proc&#x2F;net&#x2F;dev 网络流入流出的统计信息，包括接收包的数量、发送包的数量，发送数据包时的错误和冲突情况等。\n&#x2F;proc&#x2F;cmdline 在启动时传递至内核的启动参数，通常由grub启动管理工具进行传递；\n&#x2F;proc&#x2F;devices 系统已经加载的所有块设备和字符设备的信息；\n&#x2F;proc&#x2F;mounts 系统中当前挂载的所有文件系统；\n&#x2F;proc&#x2F;partitions 块设备每个分区的主设备号（major）和次设备号（minor）等信息，同时包括每个分区所包含的块（block）数目；\n&#x2F;proc&#x2F;uptime 系统上次启动以来的运行时间；\n&#x2F;proc&#x2F;version 当前系统运行的内核版本号，在作者的Debian系统中，还会显示系统安装的gcc版本；\n&#x2F;proc&#x2F;vmstat 当前系统虚拟内存的统计数据。\n\nscp命令：基于ssh远程拷贝scp 是 secure copy 的缩写, scp 是 linux 系统下基于 ssh 登陆进行安全的远程文件拷贝命令。可以结合SSH免密，并设置host地址，把IP数字设置成简短的英文名\n参数-r： 递归复制整个目录。复制目录时要加-r\nscp -r jdk1.8.0_241 root@node2:/export/server/\n\nFinalShell的命令编辑器能发送到全部会话IDEApom报错如果手动修改了本地文件，pom报错，重启一下idea，因为idea无法动态识别\n快速选中一行一、鼠标连续点三下二、end键将光标移到行尾 ， ctrl+w 选中行三、end键将光标移到行尾 ， shift + home 选中行四、home 键 光标移到行首、然后 点击shift +end\nMavenmaven插件版本不需要和maven版本对应，每个maven插件都有自己的版本个人成长PEST分析法从政治、经济、社会、技术因素去分析企业管理经营的问题。可以应用到对个人的分析\n其他命令行英文缩写CLI命令行界面（英语：command-line interface，缩写：CLI）\n几个磁盘架构知识磁盘阵列RAID2、3、4较少实际应用，它们大多只在研究领域有实作。\n◇RAID 0\n\n优点：使用 n 颗硬盘，即可拥有将近 n 倍的读写效能。\n缺点：数据安全性较低，同组数组中任一硬盘发生问题就会造成数据遗失。\n硬盘数量：最少 2 个。\n\n\n◇RAID 1\n\n优点：安全性依照数组里的实体硬盘数量倍数成长。\n缺点：空间利用率是所有 RAID 中最没有效率的。\n硬盘数量：最少 2 个。\n\n\n◇RAID 5\n\n优点：兼顾空间利用率与安全性。\n缺点：需要额外的运算资源，仅能忍受 1 个硬盘损毁。\n硬盘数量：至少 3 个。\n\n\n◇RAID 6\n\n优点：容错硬盘数量比 RAID 5 多 1 颗。\n缺点：运算量比 RAID 5 大、空间利用率比 RAID 5 低。\n硬盘数量：至少 4 个。\n\n\nDAS，NAS，SAN对比\n\n\n\n","categories":["大数据"],"tags":["大数据"]},{"title":"Keep主题需要安装的插件","url":"/2022/05/01/Keep%E4%B8%BB%E9%A2%98%E9%9C%80%E8%A6%81%E5%AE%89%E8%A3%85%E7%9A%84%E6%8F%92%E4%BB%B6/","content":"需要安装的插件\ngit一键部署：\nnpm install hexo-deployer-git --savenpm install hexo-server --save\n\n本地图片插件：\nnpm install hexo-asset-img --save\n\n开启文章字数统计和阅读时长：\nnpm install hexo-wordcount\n\n搜索功能：\nnpm install hexo-generator-searchdb\n\n RSS 订阅功能：\nnpm install hexo-generator-feed\n","categories":["博客"],"tags":["博客","Hexo","Keep主题"]},{"title":"《Linux就该这么学》笔记","url":"/2022/05/01/%E3%80%8ALinux%E5%B0%B1%E8%AF%A5%E8%BF%99%E4%B9%88%E5%AD%A6%E3%80%8B%E7%AC%94%E8%AE%B0/","content":"Linux就该这么学\n安装部署\n初始化进程服务由以前的System V init改为systemd\n命令改用systemctl来管理系统服务，如service等\n\n\n第2章 新手必须掌握的Linux命令\n两下Tab键\n\nCtrl+l组合键清屏\n\n[root@linuxprobe～]# ：当前登录用户名为root，简要的主机名是linuxprobe；这里的～是指用户家目录；#表示管理员身份（如果是$则表示普通用户，相应的权限也会小一些）\n\npstree命令用于以树状图的形式展示进程之间的关系\n\nps命令用于查看系统中的进程状态\n\npoweroff命令用于关闭系统\n\ntop命令用于动态地监视进程活动及系统负载等信息\n\npidof命令用于查询某个指定服务进程的PID号码值\n\nnice命令用于调整进程的优先级\n\n系统状态检测命令：\n\nifconfig命令用于获取网卡配置与网络状态等信息\nuname -a命令用于查看系统内核版本与系统架构等信息\nuptime命令用于查看系统的负载信息：相当于top命令第一行\nfree -h命令用于显示当前系统中内存的使用量信息：top命令第四和五行\nwho命令用于查看当前登入主机的用户终端信息\nlast命令用于调取主机的被访记录\ntracepath命令用于显示数据包到达目的主机时途中经过的所有路由信息\nnetstat命令用于显示如网络连接、路由表、接口状态等的网络相关信息\nhistory命令用于显示执行过的命令历史\nsosreport命令用于收集系统配置及架构信息并输出诊断文档\n\n\n文件位置搜索命令：\n\n“cd -”命令返回到上一次所处的目录\n“cd..”命令进入上级目录\n“cd～”命令切换到当前用户的家目录\npwd命令用于显示用户当前所处的工作目录\ntree命令用于以树状图的形式列出目录内容及结构\nfind命令用于按照指定条件来查找文件所对应的位置\n-exec …… {};\t后面可跟用于进一步处理搜索结果的命令\n\n\nlocate命令用于按照名称快速搜索文件所对应的位置，对于搜索功能来说速度快于find命令\n第一次使用locate命令之前，记得先执行updatedb命令来生成索引数据库\n\n\nwhereis命令也是基于updatedb命令所生成的索引库文件进行搜索，它与locate命令的区别是不关心那些相同名称的文件，仅仅是快速找到对应的命令文件及其帮助文件所在的位置。\nwhich命令既不关心同名文件（find与locate），也不关心命令所对应的源代码和帮助文件（whereis），仅仅是想找到命令本身所在的路径\n\n\n文本文件编辑命令：\n\ncat -n带行号查看小文本\n\nhead命令用于查看纯文本文件的前N行\n\ntail命令用于查看纯文本文件的后N行或持续刷新文件的最新内容\n\ntail -f：能够持续刷新一个文件的内容，当想要实时查看最新的日志文件时，这特别有用\n\n\ntr命令用于替换文本内容中的字符\n\nwc命令用于统计指定文本文件的行数、字数或字节数\n\n表2-14                          wc命令中的参数以及作用\n\n\n\n参数\n作用\n\n\n\n-l\n只显示行数\n\n\n-w\n只显示单词数\n\n\n-c\n只显示字节数\n\n\n\n\n\nstat命令用于查看文件的具体存储细节和时间等信息\n\ngrep命令用于按行提取文本内容\n\n-n参数用来显示搜索到的信息的行号；\n-v参数用于反选信息（即没有包含关键词的所有信息行）。\n\n\ncut命令用于按“列”提取文本内容\n\n-f参数设置需要查看的列数，还需要使用-d参数来设置间隔符号\n\n\ndiff命令用于比较多个文件之间内容的差异\n\n不仅可以使用–brief参数来确认两个文件是否相同，还可以使用-c参数来详细比较出多个文件的差异之处\n\n\nuniq命令用于去除文本中连续的重复行\n\n中间不能夹杂其他文本行（非相邻的默认不会去重）\n\n\nsort命令用于对文本内容进行再排序\n\n默认会按照字母顺序进行排序\n-u\t去除重复行:无论内容行之间是否夹杂有其他内容，只要有两个一模一样的内容行立马就可以使用-u参数进行去重操作\n-n\t以数值型排序\n\n\n\n\n文件目录管理命令\n\n“.”表示当前目录，也可以用“.&#x2F;”表示；\n\ntouch命令用于创建空白文件或设置文件的时间\n\nmkdir命令还可以结合-p参数来递归创建出具有嵌套层叠关系的文件目录:mkdir -p a&#x2F;b&#x2F;c&#x2F;d&#x2F;e\n\nmv命令用于剪切或重命名文件\n\nrm命令用于删除文件或目录\n\n-r\t递归持续（用于目录）:适用于上面两个命令\n-f\t强制执行\n-v\t显示过程\n\n\ndd命令用于按照指定大小和个数的数据块来复制文件或转换文件\n\nfile命令用于查看文件的类型\n\ntar命令用于对文件进行打包压缩或解压\n\n\n\n\n参数\n作用\n\n\n\n-c\n创建压缩文件\n\n\n-x\n解开压缩文件\n\n\n-t\n查看压缩包内有哪些文件\n\n\n-z\n用Gzip压缩或解压\n\n\n-j\n用bzip2压缩或解压\n\n\n-v\n显示压缩或解压的过程\n\n\n-f\n目标文件名\n\n\n-p\n保留原始的权限与属性\n\n\n-P\n使用绝对路径来压缩\n\n\n-C\n指定解压到的目录\n\n\n\n&#96;&#96;&#96;bash压缩tar czvf etc.tar.gz &#x2F;etc解压tar xzvf etc.tar.gz -C &#x2F;root&#x2F;etc\n3. #### 第3章 管道符、重定向与环境变量   - 表3-1                     输入重定向中用到的符号及其作用     | 符号                 | 作用                                         |     | -------------------- | -------------------------------------------- |     | 命令 &lt; 文件          | 将文件作为命令的标准输入                     |     | 命令 &lt;&lt; 分界符       | 从标准输入中读入，直到遇见分界符才停止       |     | 命令 &lt; 文件1 &gt; 文件2 | 将文件1作为命令的标准输入并将标准输出到文件2 |     表3-2                     输出重定向中用到的符号及其作用     | 符号                                | 作用                                                         |     | ----------------------------------- | ------------------------------------------------------------ |     | 命令 &gt; 文件                         | 将标准输出重定向到一个文件中（清空原有文件的数据）           |     | 命令 2&gt; 文件                        | 将错误输出重定向到一个文件中（清空原有文件的数据）           |     | 命令 &gt;&gt; 文件                        | 将标准输出重定向到一个文件中（追加到原有内容的后面）         |     | 命令 2&gt;&gt; 文件                       | 将错误输出重定向到一个文件中（追加到原有内容的后面）         |     | 命令 &gt;&gt; 文件 2&gt;&amp;1  或 命令 &amp;&gt;&gt; 文件 | 将标准输出与错误输出共同写入到文件中（追加到原有内容的后面） |   - 标准输出重定向&gt;：只对**正常输出**的内容重定向到指定文件   - 错误输出重定向2&gt;：只对**错误输出**的内容重定向到指定文件   - 不区分标准输出和错误输出，只要命令有输出信息则全部追加写入到文件中。这就要用到&amp;&gt;&gt;操作符了   - 管道命令符的作用也可以用一句话概括为“**把前一个命令原本要输出到屏幕的信息当作后一个命令的标准输入**”。   - 表3-3                    Linux系统中的通配符及含义     | 通配符      | 含义           |     | ----------- | -------------- |     | *           | 任意字符       |     | ?           | 单个任意字符   |     | [a-z]       | 单个小写字母   |     | [A-Z]       | 单个大写字母   |     | [a-Z]       | 单个字母       |     | [0-9]       | 单个数字       |     | [[:alpha:]] | 任意字母       |     | [[:upper:]] | 任意大写字母   |     | [[:lower:]] | 任意小写字母   |     | [[:digit:]] | 所有数字       |     | [[:alnum:]] | 任意字母加数字 |     | [[:punct:]] | 标点符号       |   - 通配符创建多个文件时，需要使用大括号，并且字段之间用逗号间隔：     - ```bash       touch &#123;AA,BB,CC&#125;.conf\n\n\n\n\n常用的转义字符:\n\n反斜杠（\\）：使反斜杠后面的一个变量变为单纯的字符。\n单引号（’ ‘）：转义其中所有的变量为单纯的字符串。\n双引号（” “）：保留其中的变量属性，不进行转义处理。\n反引号（` `）：把其中的命令执行后返回结果。\n\n\n\n表3-4                    Linux系统中最重要的10个环境变量\n\n\n\n变量名称\n作用\n\n\n\nHOME\n用户的主目录（即家目录）\n\n\nSHELL\n用户在使用的Shell解释器名称\n\n\nHISTSIZE\n输出的历史命令记录条数\n\n\nHISTFILESIZE\n保存的历史命令记录条数\n\n\nMAIL\n邮件保存路径\n\n\nLANG\n系统语言、语系名称\n\n\nRANDOM\n生成一个随机数字\n\n\nPS1\nBash解释器的提示符\n\n\nPATH\n定义解释器搜索用户执行命令的路径\n\n\nEDITOR\n用户默认的文本编辑器\n\n\n\n如果工作需要，可以使用export命令将其提升为全局变量，这样其他用户也就可以使用它了\n\n后续要是不使用这个变量了，则可执行unset命令把它取消掉\n\n\n\n第4章 Vim编辑器与Shell命令脚本\n在每次运行Vim编辑器时，默认进入命令模式，此时需要先切换到输入模式后再进行文档编写工作。而每次在编写完文档后需要先返回命令模式，然后再进入末行模式，执行文档的保存或退出操作。在Vim中，无法直接从输入模式切换到末行模式。\n\n表4-1                          命令模式中最常用的一些命令\n\n\n\n命令\n作用\n\n\n\ndd\n删除（剪切）光标所在整行\n\n\n5dd\n删除（剪切）从光标处开始的5行\n\n\nyy\n复制光标所在整行\n\n\n5yy\n复制从光标处开始的5行\n\n\nn\n显示搜索命令定位到的下一个字符串\n\n\nN\n显示搜索命令定位到的上一个字符串\n\n\nu\n撤销上一步的操作\n\n\np\n将之前删除（dd）或复制（yy）过的数据粘贴到光标下面的一行\n\n\n大写P\n将之前删除（dd）或复制（yy）过的数据粘贴到光标上面的一行\n\n\nG\n文件尾\n\n\ngg\n文件首\n\n\nhome\n行首\n\n\nend\n行尾\n\n\n\n表4-2                          末行模式中最常用的一些命令\n\n\n\n命令\n作用\n\n\n\n:w\n保存\n\n\n:q\n退出\n\n\n:q!\n强制退出（放弃对文档的修改内容）\n\n\n:wq!\n强制保存退出（等于在命令模式下按❗shift+zz❗）\n\n\n:set nu\n显示行号\n\n\n:set nonu\n不显示行号\n\n\n:命令\n执行该命令\n\n\n:整数\n跳转到该行\n\n\n:s&#x2F;one&#x2F;two\n将当前光标所在行的第一个one替换成two\n\n\n:s&#x2F;one&#x2F;two&#x2F;g\n将当前光标所在行的所有one替换成two\n\n\n:%s&#x2F;one&#x2F;two&#x2F;g\n将全文中的所有one替换成two\n\n\n?字符串\n在文本中从下至上搜索该字符串\n\n\n&#x2F;字符串\n在文本中从上至下搜索该字符串\n\n\n\n命令模式切换到输入模式，a键与i键分别是在光标后面一位和光标当前位置切换到输入模式，而o键则是在光标的下面再创建一个空行\n\n变量之间使用空格间隔。例如，$0对应的是当前Shell脚本程序的名称，$#对应的是总共有几个参数，$*对应的是所有位置的参数值，$?对应的是显示上一次命令的执行返回值，而$1、$2、$3……则分别对应着第N个位置的参数值\n\nShell脚本中的条件测试语法可以判断表达式是否成立，若条件成立则返回数字0，否则便返回非零值。\n\n\n切记，条件表达式两边均应有一个空格。\n\n\n条件测试语句可以分为4种：\n\n文件测试语句；\n逻辑测试语句；\n整数值比较语句；\n字符串比较语句。\n\n\n表4-3                          文件测试所用的参数\n\n\n\n操作符\n作用\n\n\n\n-d\n测试文件是否为目录类型\n\n\n-e\n测试文件是否存在\n\n\n-f\n判断是否为一般文件\n\n\n-r\n测试当前用户是否有权限读取\n\n\n-w\n测试当前用户是否有权限写入\n\n\n-x\n测试当前用户是否有权限执行\n\n\n\n逻辑“与”的运算符号是&amp;&amp;，它表示当前面的命令执行成功后才会执行它后面的命令\n\n逻辑“或”||，表示当前面的命令执行失败后才会执行它后面的命令\n\n第三种逻辑语句是“非”，在Linux系统中的运算符号是一个叹号（！），它表示把条件测试中的判断结果取相反值。\n\n表4-4                         可用的整数比较运算符\n\n\n\n操作符\n作用\n\n\n\n-eq\n是否等于\n\n\n-ne\n是否不等于\n\n\n-gt\n是否大于\n\n\n-lt\n是否小于\n\n\n-le\n是否等于或小于\n\n\n-ge\n是否大于或等于\n\n\n\n表4-5                        常见的字符串比较运算符\n\n\n\n操作符\n作用\n\n\n\n&#x3D;\n比较字符串内容是否相同\n\n\n!&#x3D;\n比较字符串内容是否不同\n\n\n-z\n判断字符串内容是否为空\n\n\n\n流程控制语句：\n\nif…then…else…elif…fi\n\n\n运行SHELL脚本的两种方式：\n\nbash 脚本\n.&#x2F;脚本 通过完整路径\n\n\n在Linux系统中，read是用来读取用户输入信息的命令，能够把接收到的用户输入信息赋值给后面的指定变量，-p参数用于向用户显示一些提示信息。\n\nread -p &quot;Enter your score（0-100）：&quot; GRADE\n   - /dev/null是一个被称作Linux黑洞的文件，把输出信息重定向到这个文件等同于删除数据（类似于没有回收功能的垃圾箱），可以让用户的屏幕窗口保持简洁。   - $()等价于反引号（\\` \\`）：把其中的命令执行后返回结果。   - 计划任务服务程序:     1. 一次性计划任务：可以用at命令实现这种功能，只需要写成“at时间”的形式就行。如果想要查看已设置好但还未执行的一次性计划任务，可以使用at -l命令；要想将其**删除**，可以使用“**atrm**任务序号”。        - 表4-6                        at命令的参数及其作用          | 参数 | 作用                   |          | ---- | ---------------------- |          | -f   | 指定包含命令的任务文件 |          | -q   | 指定新任务名称         |          | -l   | 显示待执行任务列表     |          | -d   | 删除指定待执行任务     |          | -m   | 任务执行后给用户发邮件 |        - 把计划任务写入Shell脚本中，当用户激活该脚本后再开始倒计时执行，而不是像上面那样在固定的时间（“at 23:30”命令）进行。一般我们会使用“at now +2 MINUTE”的方式进行操作，这表示2分钟（MINUTE）后执行这个任务，也可以将其替代成小时（HOUR）、日（DAY）、月（MONTH）等词汇     2. 长期性计划任务：那么Linux系统中默认启用的crond服务简直再适合不过了。创建、编辑计划任务的命令为crontab -e，查看当前计划任务的命令为crontab -l，删除某条计划任务的命令为crontab -r。另外，如果您是以管理员的身份登录的系统，还可以在crontab命令中加上-u参数来编辑他人的计划任务。        - 表4-7                        crontab命令的参数及其作用          | 参数 | 作用         |          | ---- | ------------ |          | -e   | 编辑计划任务 |          | -u   | 指定用户名称 |          | -l   | 列出任务列表 |          | -r   | 删除计划任务 |        - 在正式部署计划任务前，请先跟刘遄老师念一下口诀“分、时、日、月、星期 命令”。这是使用crond服务设置任务的参数格式（其格式见表4-8）。需要注意的是，如果有些字段没有被设置，则需要使用星号（*****）占位，如图4-24所示。![第4章 Vim编辑器与Shell命令脚本第4章 Vim编辑器与Shell命令脚本](https://www.linuxprobe.com/wp-content/uploads/2015/02/cron%E8%AE%A1%E5%88%92%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%8F%82%E6%95%B0.png)        - 除了用逗号（,）来分别表示多个时间段，例如“8,9,12”表示8月、9月和12月。还可以用减号（-）来表示一段连续的时间周期（例如字段“日”的取值为“12-15”，则表示每月的12～15日）。还可以用除号（/）表示执行任务的间隔时间（例如“*/2”表示每隔2分钟执行一次任务）        - 如果在crond服务中需要同时包含多条计划任务的命令语句，应每行仅写一条。        - 尤其需要注意的是，在crond服务的计划任务参数中，所有命令一定要用绝对路径的方式来写，如果不知道绝对路径，请用whereis命令进行查询。        - 在crond服务的配置参数中，一般会像Shell脚本那样以#号开头写上注释信息，这样在日后回顾这段命令代码时可以快速了解其功能、需求以及编写人员等重要信息。        - 计划任务中的“分”字段必须有数值，绝对不能为空或是*号，而“日”和“星期”字段不能同时使用，否则就会发生冲突。5. #### 第5章 用户身份与文件权限   - &gt; **管理员UID为0**：系统的管理员用户。     &gt;     &gt; **系统用户UID为1～999**：Linux系统为了避免因某个服务程序出现漏洞而被黑客提权至整台服务器，默认服务程序会由独立的系统用户负责运行，进而有效控制被破坏范围。     &gt;     &gt; **普通用户UID从1000开始**：是由管理员创建的用于日常工作的用户。   - **id命令**:“id用户名”     - id命令用于显示用户的详细信息   - **useradd命令**:“useradd [参数] 用户名”     - useradd命令用于创建新的用户账户     - 表5-1                    useradd命令中的参数以及作用       | 参数 | 作用                                     |       | ---- | ---------------------------------------- |       | -d   | 指定用户的家目录（默认为/home/username） |       | -e   | 账户的到期时间，格式为YYYY-MM-DD.        |       | -u   | 指定该用户的默认UID                      |       | -g   | 指定一个初始的用户基本组（必须已存在）   |       | -G   | 指定一个或多个扩展用户组                 |       | -N   | 不创建与用户同名的基本用户组             |       | -s   | 指定该用户的默认Shell解释器              |   - **groupadd命令**:“groupadd [参数] 群组名”     - groupadd命令用于创建新的用户组   - **usermod命令**:“usermod [参数] 用户名”     - usermod命令用于修改用户的属性。用户的信息保存在/etc/passwd文件中，可以直接用文本编辑器来修改其中的用户参数项目，也可以用usermod命令修改已经创建的用户信息。     - 表5-2                      usermod命令中的参数以及作用       | 参数  | 作用                                                         |       | ----- | ------------------------------------------------------------ |       | -c    | 填写用户账户的备注信息                                       |       | -d -m | 参数-m与参数-d连用，可重新指定用户的家目录并自动把旧的数据转移过去 |       | -e    | 账户的到期时间，格式为YYYY-MM-DD                             |       | -g    | 变更所属用户组                                               |       | -G    | 变更扩展用户组                                               |       | -L    | 锁定用户禁止其登录系统                                       |       | -U    | 解锁用户，允许其登录系统                                     |       | -s    | 变更默认终端                                                 |       | -u    | 修改用户的UID                                                |   - **passwd命令**：“passwd [参数] 用户名”     - root管理员在Linux系统中修改自己或他人的密码时不需要验证旧密码，这一点特别方便。既然root管理员能够修改其他用户的密码，就表示其完全拥有该用户的管理权限。     - 表5-3                      passwd命令中的参数以及作用       | 参数    | 作用                                                         |       | ------- | ------------------------------------------------------------ |       | -l      | 锁定用户，禁止其登录                                         |       | -u      | 解除锁定，允许用户登录                                       |       | --stdin | 允许通过标准输入修改用户密码，如echo &quot;NewPassWord&quot; \\| passwd --stdin Username |       | -d      | 使该用户可用空密码登录系统                                   |       | -e      | 强制用户在下次登录时修改密码                                 |       | -S      | 显示用户的密码是否被锁定，以及密码所采用的加密算法名称       |   - **userdel命令**：“userdel [参数] 用户名”     - userdel命令用于删除已有的用户账户。在执行删除操作时，该用户的家目录默认会保留下来，此时可以使用-r参数将其删除。在删除一个用户时，一般会建议保留他的家目录数据，以免有重要的数据被误删除。     - 表5-4                       userdel命令中的参数以及作用       | 参数 | 作用                     |       | ---- | ------------------------ |       | -f   | 强制删除用户             |       | -r   | 同时删除用户及用户家目录 |   - ##### **文件权限与归属**     - 对于一般文件来说，权限比较容易理解：“可读”表示能够读取文件的实际内容；“可写”表示能够编辑、新增、修改、删除文件的实际内容     - 对于目录文件来说，“可读”表示能够读取目录内的文件列表；“可写”表示能够在目录内新增、删除、重命名文件；而“可执行”则表示能够进入该目录。     - 表5-5         读写执行权限对于文件与目录可执行命令的区别       ![第5章 用户身份与文件权限第5章 用户身份与文件权限](https://www.linuxprobe.com/wp-content/uploads/2020/05/%E8%AF%BB%E5%86%99%E6%89%A7%E8%A1%8C%E6%9D%83%E9%99%90%E5%AF%B9%E4%BA%8E%E6%96%87%E4%BB%B6%E4%B8%8E%E7%9B%AE%E5%BD%95%E7%9A%84%E4%BD%9C%E7%94%A8-1024x168.png)     - 表5-6                       文件权限的字符与数字表示       ![第5章 用户身份与文件权限第5章 用户身份与文件权限](https://www.linuxprobe.com/wp-content/uploads/2020/05/%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%8E%E6%95%B0%E5%AD%97%E8%A1%A8%E7%A4%BA-1024x201.png)     - 常见的文件类型包括普通文件（-）、目录文件（d）、链接文件（l）、管道文件（p）、块设备文件（b）以及字符设备文件（c）。   - ##### **文件的特殊权限**     - **SUID**：能够让二进制程序的执行者临时拥有所有者的权限       - 权限由rwx变成了rws，其中x改变成s就意味着该文件被赋予了SUID权限。那么如果原本的权限是rw-呢？如果原先权限位上没有x执行权限，那么被赋予特殊权限后将变成大写的S。     - **SGID**：       - 当对二进制程序进行设置时，能够让执行者临时获取文件所属组的权限       - 当对目录进行设置时，则是让目录内新创建的文件自动继承该目录原有用户组的名称。     - chmod命令用于设置文件的一般权限及特殊权限，语法格式为“chmod [参数] 文件名”。       - ```bash         chmod 760 anaconda-ks.cfg          chmod -R g+s testdir\n\n- 使用chmod命令设置特殊权限的参数如表5-7所示。\n\n- 表5-7                     SUID、SGID、SBIT特殊权限的设置参数\n\n  | 参数 | 作用         |\n  | ---- | ------------ |\n  | u+s  | 设置SUID权限 |\n  | u-s  | 取消SUID权限 |\n  | g+s  | 设置SGID权限 |\n  | g-s  | 取消SGID权限 |\n  | o+t  | 设置SBIT权限 |\n  | o-t  | 取消SBIT权限 |\n\n\nchown命令用于设置文件的所有者和所有组，语法格式为“chown所有者:所有组 文件名”。\n\nchown linuxprobe:linuxprobe anaconda-ks.cfg \n  - chmod和chown命令是用于修改文件属性和权限的最常用命令，它们还有一个特别的共性，就是针对目录进行操作时需要加上**大写参数-R**来表示递归操作，即对目录内所有的文件进行整体操作。  - **SBIT**:SBIT特殊权限位可确保用户只能删除自己的文件，而不能删除其他用户的文件。换句话说，当对某个目录设置了SBIT粘滞位权限后，那么该目录中的文件就只能被其所有者执行删除操作了。    - 当目录被设置SBIT特殊权限位后，文件的其他用户权限部分的x执行权限就会被替换成t或者T—原本有x执行权限则会写成t，原本没有x执行权限则会被写成T。    - 文件能否被删除并不取决于自身的权限，而是看其所在目录是否有写入权限  - 数字表示法是由“特殊权限+一般权限”构成的。SUID、SGID与SBIT也有对应的数字表示法，分别为4、2、1。    - 如果权限是“rwsrwSr--”呢？首先不要慌，大写S表示原先没有执行权限，因此一般权限为rwxrw-r--，将其转换为数字表示法后结果是764。带有的SUID和SGID特殊权限的数字法表示是4和2，心算得出结果是6，合并后的结果为6764。- ##### **文件的隐藏属性**：  - 隐藏权限，即被隐藏起来的权限，默认情况下不能直接被用户发觉。  1. **chattr命令**：chattr命令用于设置文件的隐藏权限，语法格式为“chattr [参数] 文件名称”     - 如果想要把某个隐藏功能添加到文件上，则需要在命令后面追加“+参数”，如果想要把某个隐藏功能移出文件，则需要追加“-参数”。     - 表5-8                 chattr命令中的参数及其作用       | 参数 | 作用                                                         |       | ---- | ------------------------------------------------------------ |       | i    | 无法对文件进行修改；若对目录设置了该参数，则仅能修改其中的子文件内容而不能新建或删除文件 |       | a    | 仅允许补充（追加）内容，无法覆盖/删除内容（Append Only）     |       | S    | 文件内容在变更后立即同步到硬盘（sync）                       |       | s    | 彻底从硬盘中删除，不可恢复（用0填充原文件所在硬盘区域）      |       | A    | 不再修改这个文件或目录的最后访问时间（atime）                |       | b    | 不再修改文件或目录的存取时间                                 |       | D    | 检查压缩文件中的错误                                         |       | d    | 使用dump命令备份时忽略本文件/目录                            |       | c    | 默认将文件或目录进行压缩                                     |       | u    | 当删除该文件后依然保留其在硬盘中的数据，方便日后恢复         |       | t    | 让文件系统支持尾部合并（tail-merging）                       |       | x    | 可以直接访问压缩文件中的内容                                 |  2. **lsattr命令**：lsattr命令用于查看文件的隐藏权限，英文全称为“list attributes”，语法格式为“lsattr [参数] 文件名称”。- ##### **文件访问控制列表**  1. **setfacl命令**：     - setfacl命令用于管理文件的ACL权限规则，英文全称为“set files ACL”，语法格式为“setfacl [参数] 文件名称”。     - 针对目录文件需要使用-R递归参数；针对普通文件则使用-m参数；如果想要删除某个文件的ACL，则可以使用-b参数。     - 表5-9                       setfacl命令中的参数以及作用       | 参数 | 作用             |       | ---- | ---------------- |       | -m   | 修改权限         |       | -M   | 从文件中读取权限 |       | -x   | 删除某个权限     |       | -b   | 删除全部权限     |       | -R   | 递归子目录       |     - ```bash       setfacl -Rm u:linuxprobe:rwx /root\n\n\n常用的ls命令是看不到ACL信息的，但是却可以看到文件权限的最后一个点（**.）变成了加号（+**），这就意味着该文件已经设置了ACL。\n\n&#96;&#96;&#96;bash[root@linuxprobe ~]# ls -ld &#x2F;rootdr-xrwx—+ 14 root root 4096 May 4 2020 &#x2F;root   - 要清空所有ACL权限，请用-b参数；要删除某一条指定的权限，就用-x参数2. **getfacl命令**   - getfacl命令用于查看文件的ACL权限规则，英文全称为“get files ACL”，语法格式为“getfacl [参数] 文件名称”。3. **操作前备份一下，总是好的习惯**   - getfacl在备份目录权限时不能使用绝对路径的形式，因此我们需要先切换到最上层根目录，然后再进行操作。输出重定向操作，可以轻松实现权限的备份     - ```bash       [root@linuxprobe ~]# cd /       [root@linuxprobe /]# getfacl -R home &gt; backup.acl       [root@linuxprobe /]# ls -l        -rw-r--r--. 1 root root 834 Jul 18 14:14 backup.acl\n\n\nACL权限的恢复也很简单，使用的是–restore参数。由于在备份时已经指定是对&#x2F;home目录进行操作，所以不需要写对应的目录名称，它能够自动找到要恢复的对象\n\n&#96;&#96;&#96;bash[root@linuxprobe &#x2F;]# setfacl –restore backup.acl- ##### **su命令与sudo服务**  1. su:     - ```bash       [root@linuxprobe ~]# su - linuxprobe\n\nsu命令与用户名之间有一个减号（-），这意味着完全切换到新的用户，即把环境变量信息也变更为新用户的相应信息，而不是保留原始的信息。强烈建议在切换用户身份时添加这个减号（-）。\n\n当从root管理员切换到普通用户时是不需要密码验证的，而从普通用户切换成root管理员就需要进行密码验证了\n\n\n\n\n\nsudo:\n\nsudo命令用于给普通用户提供额外的权限，语法格式为“sudo [参数] 用户名”。\n\n授权原则：在保证普通用户完成相应工作的前提下，尽可能少地赋予额外的权限。\n\n表5-10                     sudo命令中的可用参数以及作用\n\n\n\n参数\n作用\n\n\n\n-h\n列出帮助信息\n\n\n-l\n列出当前用户可执行的命令\n\n\n-u 用户名或UID值\n以指定的用户身份执行命令\n\n\n-k\n清空密码的有效时间，下次执行sudo时需要再次进行密码验证\n\n\n-b\n在后台执行指定的命令\n\n\n-p\n更改询问密码的提示语\n\n\n\nvisudo命令用于编辑、配置用户sudo的权限文件，语法格式为“visudo [参数]”。visudo命令只有root管理员才可以执行\n\n在配置权限文件时，按照下面的格式在第101行（大约）填写上指定的信息。\n\n谁可以使用 允许使用的主机 &#x3D; （以谁的身份） 可执行命令的列表\n谁可以使用：稍后要为哪位用户进行命令授权。\n允许使用的主机：可以填写ALL表示不限制来源的主机，亦可填写如192.168.10.0&#x2F;24这样的网段限制来源地址，使得只有从允许网段登录时才能使用sudo命令。\n以谁的身份：可以填写ALL表示系统最高权限，也可以是另外一位用户的名字。\n可执行命令的列表：可以填写ALL表示不限制命令，亦可填写如&#x2F;usr&#x2F;bin&#x2F;cat这样的文件名称来限制命令列表，多个命令文件之间用逗号（,）间隔。\n\n\n如果需要让某个用户只能使用root管理员的身份执行指定的命令，切记一定要给出该命令的绝对路径，否则系统会识别不出来。这时，可以先使用whereis命令找出命令所对应的保存路径\n\n每次执行sudo命令都要输入一次密码其实也挺麻烦的，这时可以添加NOPASSWD参数，使得用户下次再执行sudo命令时就不用密码验证\n\nlinuxprobe ALL=(ALL) NOPASSWD:/usr/bin/cat,/usr/sbin/reboot\n\n\n\n\n\n\n\n\n\n\n\n\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"痤疮","url":"/2022/05/01/%E7%97%A4%E7%96%AE/","content":"痤疮饮食为主要因素：油腻辛辣甜食\n男性：58.9%\nBMI高,影响不大\n&gt;24 53.1% &lt;18.5 42.1%\n油性皮肤68.5%\n控油类护肤会极大减少痤疮发生\n家族史73.1%\n不爱喝水饮水量小于500ml 60.3%\n吃油腻多三倍，吃辛辣多半倍\n24点后入睡58.2%，22点前入睡患病率极低19%，睡眠越少越高，但影响不大\n电子产品与痤疮无关\n与吸烟有关，但与饮酒无关\n排序：\n油性皮肤\n喜食油腻\n家族史\n入睡时间\n","categories":["健康"],"tags":["健康"]}]